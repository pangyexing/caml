{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 客户转化模型 - 执行流程\n",
    "\n",
    "这个笔记本提供了执行 `main.py` 中各个功能模块的交互式界面，包括：\n",
    "\n",
    "1. **检查特征文件** - 检查特征文件是否有重复\n",
    "2. **合并特征文件** - 合并多个特征文件\n",
    "3. **训练模型** - 训练客户转化模型\n",
    "4. **超参数调优** - 优化模型参数\n",
    "5. **部署模型** - 部署模型并进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 环境设置\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional, Union, Any\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "notebook_dir = Path.cwd()\n",
    "project_dir = notebook_dir.parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "# 导入项目模块\n",
    "from src.core.preprocessing import (\n",
    "    check_feature_files,\n",
    "    merge_feature_files,\n",
    "    preprocess_data,\n",
    ")\n",
    "from src.models.deployment import batch_prediction, deploy_model, load_feature_list\n",
    "from src.models.hyperopt_tuning import hyperopt_xgb, plot_optimization_results\n",
    "from src.models.training import two_stage_modeling_pipeline\n",
    "\n",
    "# 设置可视化参数\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 显示项目数据目录结构\n",
    "def list_directory(path, indent=0):\n",
    "    \"\"\"列出目录结构\"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"路径不存在: {path}\")\n",
    "        return\n",
    "    \n",
    "    items = list(path.iterdir())\n",
    "    for item in sorted(items, key=lambda x: (not x.is_dir(), x.name)):\n",
    "        if item.name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        prefix = \"    \" * indent\n",
    "        if item.is_dir():\n",
    "            print(f\"{prefix}📁 {item.name}/\")\n",
    "            if indent < 2:  # 限制递归深度\n",
    "                list_directory(item, indent + 1)\n",
    "        else:\n",
    "            size_mb = item.stat().st_size / (1024 * 1024)\n",
    "            if size_mb > 1:\n",
    "                size_str = f\"{size_mb:.1f}MB\"\n",
    "            else:\n",
    "                size_kb = item.stat().st_size / 1024\n",
    "                size_str = f\"{size_kb:.1f}KB\"\n",
    "            print(f\"{prefix}📄 {item.name} ({size_str})\")\n",
    "\n",
    "print(\"项目目录结构:\")\n",
    "list_directory(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 检查特征文件\n",
    "\n",
    "检查特征文件是否有重复特征或其他问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_check_features(feature_files=None):\n",
    "    \"\"\"检查特征文件\"\"\"\n",
    "    if feature_files is None:\n",
    "        # 如果未提供文件列表，尝试查找数据目录中的所有CSV文件\n",
    "        data_dir = project_dir / \"data\"\n",
    "        if data_dir.exists():\n",
    "            feature_files = list(data_dir.glob(\"*.csv\"))\n",
    "            feature_files = [str(f) for f in feature_files]\n",
    "            print(f\"找到 {len(feature_files)} 个特征文件:\")\n",
    "            for f in feature_files:\n",
    "                print(f\"  - {f}\")\n",
    "        else:\n",
    "            print(f\"数据目录不存在: {data_dir}\")\n",
    "            return\n",
    "    \n",
    "    if not feature_files:\n",
    "        print(\"没有找到特征文件\")\n",
    "        return\n",
    "    \n",
    "    print(f\"检查 {len(feature_files)} 个特征文件是否有重复...\")\n",
    "    results = check_feature_files(feature_files)\n",
    "    \n",
    "    if results['status'] == 'success':\n",
    "        print(\"✅ 没有发现问题\")\n",
    "    else:\n",
    "        print(f\"⚠️ 发现 {len(results['issues'])} 个问题:\")\n",
    "        for issue in results['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 运行特征文件检查\n",
    "# 如果需要指定文件，请替换下面的None\n",
    "feature_files = None  # 例如: [\"data/feature1.csv\", \"data/feature2.csv\"]\n",
    "check_results = run_check_features(feature_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 合并特征文件\n",
    "\n",
    "将多个特征文件合并为一个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_merge_features(feature_files=None, sample_file=None, output_file=None):\n",
    "    \"\"\"合并特征文件\"\"\"\n",
    "    if feature_files is None:\n",
    "        # 如果未提供文件列表，尝试查找数据目录中的所有CSV文件\n",
    "        data_dir = project_dir / \"data\"\n",
    "        if data_dir.exists():\n",
    "            feature_files = list(data_dir.glob(\"*.csv\"))\n",
    "            feature_files = [str(f) for f in feature_files]\n",
    "            print(f\"找到 {len(feature_files)} 个特征文件:\")\n",
    "            for f in feature_files:\n",
    "                print(f\"  - {f}\")\n",
    "        else:\n",
    "            print(f\"数据目录不存在: {data_dir}\")\n",
    "            return\n",
    "    \n",
    "    if not feature_files:\n",
    "        print(\"没有找到特征文件\")\n",
    "        return\n",
    "    \n",
    "    print(f\"合并 {len(feature_files)} 个特征文件...\")\n",
    "    merged_df = merge_feature_files(feature_files, sample_file)\n",
    "    \n",
    "    # 保存合并的数据集\n",
    "    if output_file is None:\n",
    "        output_file = str(project_dir / \"merged_features.csv\")\n",
    "    \n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ 已将合并后的数据集保存到 {output_file}\")\n",
    "    print(f\"数据集大小: {len(merged_df)} 行 x {len(merged_df.columns)} 列\")\n",
    "    \n",
    "    # 显示一些基本统计信息\n",
    "    print(\"\\n数据集概览:\")\n",
    "    display(merged_df.head())\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 运行特征文件合并\n",
    "# 如需指定特定文件，请修改下面的参数\n",
    "feature_files = None  # 例如: [\"data/feature1.csv\", \"data/feature2.csv\"]\n",
    "sample_file = None    # 例如: \"data/sample.csv\"\n",
    "output_file = None    # 例如: \"merged_data.csv\"\n",
    "\n",
    "merged_df = run_merge_features(feature_files, sample_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练模型\n",
    "\n",
    "训练客户转化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_train_model(data_file=None, target='label_apply', resume_from=None):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    if data_file is None:\n",
    "        # 如果未提供数据文件，尝试使用合并的特征文件\n",
    "        default_data = project_dir / \"merged_features.csv\"\n",
    "        if default_data.exists():\n",
    "            data_file = str(default_data)\n",
    "        else:\n",
    "            print(f\"数据文件不存在: {default_data}\")\n",
    "            print(\"请先合并特征文件或指定数据文件路径\")\n",
    "            return\n",
    "    \n",
    "    print(f\"正在从 {data_file} 加载数据...\")\n",
    "    \n",
    "    # 加载数据\n",
    "    if data_file.endswith('.csv'):\n",
    "        df = pd.read_csv(data_file)\n",
    "    elif data_file.endswith('.parquet'):\n",
    "        df = pd.read_parquet(data_file)\n",
    "    else:\n",
    "        print(f\"错误: 不支持的文件格式: {data_file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"数据加载完成，大小: {df.shape}\")\n",
    "    \n",
    "    # 检查目标变量\n",
    "    if target not in df.columns:\n",
    "        print(f\"错误: 目标变量 '{target}' 不在数据集中\")\n",
    "        print(f\"可用列: {', '.join(df.columns[:10])}...\")\n",
    "        return\n",
    "    \n",
    "    # 预处理和拆分数据\n",
    "    print(\"预处理数据...\")\n",
    "    train_df, test_df = preprocess_data(df, target=target)\n",
    "    \n",
    "    print(f\"训练集大小: {train_df.shape}\")\n",
    "    print(f\"测试集大小: {test_df.shape}\")\n",
    "    \n",
    "    # 运行两阶段建模流程\n",
    "    print(\"开始训练模型...\")\n",
    "    results = two_stage_modeling_pipeline(\n",
    "        train_df, \n",
    "        test_df, \n",
    "        target=target,\n",
    "        resume_from=resume_from\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 训练完成\")\n",
    "    \n",
    "    # 显示结果摘要\n",
    "    if isinstance(results, dict) and 'metrics' in results:\n",
    "        print(\"\\n模型评估指标:\")\n",
    "        for metric, value in results['metrics'].items():\n",
    "            print(f\"  - {metric}: {value}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 运行模型训练\n",
    "# 可修改以下参数\n",
    "data_file = None  # 例如: \"merged_features.csv\"\n",
    "target = 'label_apply'  # 目标变量列名\n",
    "resume_from = None  # 可选: 'start', 'initial_model', 'feature_analysis', 'feature_selection', 'final_model'\n",
    "\n",
    "training_results = run_train_model(data_file, target, resume_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 超参数调优\n",
    "\n",
    "对模型进行超参数优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_hyperparameter_tuning(data_file=None, target='label_apply', max_evals=50):\n",
    "    \"\"\"超参数调优\"\"\"\n",
    "    # 检查特征文件是否存在\n",
    "    features_file = project_dir / \"funnel_models\" / \"selected_features.txt\"\n",
    "    \n",
    "    if not features_file.exists():\n",
    "        print(f\"错误: 特征列表文件不存在: {features_file}\")\n",
    "        print(\"请先运行训练模式\")\n",
    "        return\n",
    "    \n",
    "    # 决定是加载原始数据还是使用已有的训练/测试数据\n",
    "    if data_file:\n",
    "        print(f\"从 {data_file} 加载数据...\")\n",
    "        \n",
    "        # 加载数据\n",
    "        if data_file.endswith('.csv'):\n",
    "            df = pd.read_csv(data_file)\n",
    "        elif data_file.endswith('.parquet'):\n",
    "            df = pd.read_parquet(data_file)\n",
    "        else:\n",
    "            print(f\"错误: 不支持的文件格式: {data_file}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"数据加载完成，大小: {df.shape}\")\n",
    "        \n",
    "        # 预处理和拆分数据\n",
    "        print(\"预处理数据...\")\n",
    "        train_df, test_df = preprocess_data(df, target=target)\n",
    "    else:\n",
    "        print(\"尝试从 funnel_models 目录加载已处理的数据...\")\n",
    "        \n",
    "        # 尝试加载保存的文件\n",
    "        train_file = project_dir / \"funnel_models\" / \"train.csv\"\n",
    "        test_file = project_dir / \"funnel_models\" / \"test.csv\"\n",
    "        \n",
    "        if not train_file.exists() or not test_file.exists():\n",
    "            print(f\"错误: 训练数据文件不存在: {train_file} 或 {test_file}\")\n",
    "            print(\"请指定数据文件路径或先运行训练模式\")\n",
    "            return\n",
    "        \n",
    "        train_df = pd.read_csv(train_file)\n",
    "        test_df = pd.read_csv(test_file)\n",
    "        \n",
    "        print(f\"数据加载完成，训练集: {train_df.shape}, 测试集: {test_df.shape}\")\n",
    "    \n",
    "    # 加载特征列表\n",
    "    features = load_feature_list(str(features_file))\n",
    "    print(f\"从 {features_file} 加载了 {len(features)} 个特征\")\n",
    "    \n",
    "    # 运行超参数优化\n",
    "    print(f\"开始超参数优化 (最大评估次数: {max_evals})...\")\n",
    "    results = hyperopt_xgb(\n",
    "        train_df,\n",
    "        test_df,\n",
    "        features,\n",
    "        target=target,\n",
    "        max_evals=max_evals\n",
    "    )\n",
    "    \n",
    "    # 创建可视化图表\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_dir = project_dir / \"optimization_results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    results_file = results_dir / f\"{target}_{timestamp}_results.json\"\n",
    "    print(f\"绘制优化结果图表，保存到 {results_file}...\")\n",
    "    plot_optimization_results(str(results_file))\n",
    "    \n",
    "    print(\"✅ 超参数调优完成\")\n",
    "    \n",
    "    # 显示最佳参数\n",
    "    if 'best_params' in results:\n",
    "        print(\"\\n最佳参数:\")\n",
    "        for param, value in results['best_params'].items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "        \n",
    "        if 'best_score' in results:\n",
    "            print(f\"\\n最佳得分: {results['best_score']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 运行超参数调优\n",
    "# 可修改以下参数\n",
    "data_file = None  # 如果不指定，将使用 funnel_models 中的训练/测试数据\n",
    "target = 'label_apply'  # 目标变量列名\n",
    "max_evals = 50  # 最大评估次数，可根据需要调整\n",
    "\n",
    "tuning_results = run_hyperparameter_tuning(data_file, target, max_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 部署模型\n",
    "\n",
    "部署模型并进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_model_deployment(model_file=None, data_file=None, target=None, features_file=None, \n",
    "                         key_column='input_key', output_file=None, threshold=0.5):\n",
    "    \"\"\"部署模型\"\"\"\n",
    "    # 检查模型文件\n",
    "    if model_file is None:\n",
    "        # 尝试查找最新的模型文件\n",
    "        model_dir = project_dir / \"funnel_models\"\n",
    "        if model_dir.exists() and (model_dir / \"final_model.pkl\").exists():\n",
    "            model_file = str(model_dir / \"final_model.pkl\")\n",
    "        else:\n",
    "            # 也可以查找tuned_models目录\n",
    "            tuned_dir = project_dir / \"tuned_models\"\n",
    "            if tuned_dir.exists():\n",
    "                model_files = list(tuned_dir.glob(\"*.pkl\"))\n",
    "                if model_files:\n",
    "                    # 按修改时间排序，选择最新的\n",
    "                    model_file = str(sorted(model_files, key=lambda x: x.stat().st_mtime, reverse=True)[0])\n",
    "    \n",
    "    if model_file is None or not os.path.exists(model_file):\n",
    "        print(f\"错误: 模型文件不存在\")\n",
    "        print(\"请先训练模型或指定模型文件路径\")\n",
    "        return\n",
    "    \n",
    "    print(f\"使用模型: {model_file}\")\n",
    "    \n",
    "    # 检查数据文件\n",
    "    if data_file is None:\n",
    "        # 尝试使用合并的特征文件\n",
    "        default_data = project_dir / \"merged_features.csv\"\n",
    "        if default_data.exists():\n",
    "            data_file = str(default_data)\n",
    "        else:\n",
    "            print(f\"错误: 数据文件不存在\")\n",
    "            print(\"请指定数据文件路径\")\n",
    "            return\n",
    "    \n",
    "    print(f\"从 {data_file} 加载数据...\")\n",
    "    \n",
    "    # 加载数据\n",
    "    if data_file.endswith('.csv'):\n",
    "        df = pd.read_csv(data_file)\n",
    "    elif data_file.endswith('.parquet'):\n",
    "        df = pd.read_parquet(data_file)\n",
    "    else:\n",
    "        print(f\"错误: 不支持的文件格式: {data_file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"数据加载完成，大小: {df.shape}\")\n",
    "    \n",
    "    # 检查特征文件\n",
    "    if features_file is None:\n",
    "        default_features = project_dir / \"funnel_models\" / \"selected_features.txt\"\n",
    "        if default_features.exists():\n",
    "            features_file = str(default_features)\n",
    "        else:\n",
    "            print(f\"错误: 特征列表文件不存在\")\n",
    "            print(\"请指定特征列表文件路径\")\n",
    "            return\n",
    "    \n",
    "    # 部署模型\n",
    "    if target and target in df.columns:\n",
    "        print(f\"部署模型并对 {target} 进行评估...\")\n",
    "        \n",
    "        results = deploy_model(\n",
    "            model_file,\n",
    "            df,\n",
    "            target=target,\n",
    "            features_file=features_file,\n",
    "            threshold=threshold\n",
    "        )\n",
    "        \n",
    "        # 显示评估结果\n",
    "        if isinstance(results, dict) and 'metrics' in results:\n",
    "            print(\"\\n模型评估指标:\")\n",
    "            for metric, value in results['metrics'].items():\n",
    "                print(f\"  - {metric}: {value}\")\n",
    "    else:\n",
    "        print(\"部署模型进行批量预测...\")\n",
    "        \n",
    "        if output_file is None:\n",
    "            # 设置默认输出文件\n",
    "            deploy_dir = project_dir / \"deployment_results\"\n",
    "            deploy_dir.mkdir(exist_ok=True)\n",
    "            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_file = str(deploy_dir / f\"predictions_{timestamp}.csv\")\n",
    "        \n",
    "        results = batch_prediction(\n",
    "            model_file,\n",
    "            df,\n",
    "            key_column=key_column,\n",
    "            features_file=features_file,\n",
    "            output_file=output_file,\n",
    "            threshold=threshold\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 预测完成，结果保存到 {output_file}\")\n",
    "        \n",
    "        # 显示预测结果预览\n",
    "        if isinstance(results, pd.DataFrame):\n",
    "            print(\"\\n预测结果预览:\")\n",
    "            display(results.head())\n",
    "            \n",
    "            # 统计预测情况\n",
    "            if 'prediction' in results.columns:\n",
    "                pos_count = results['prediction'].sum()\n",
    "                total = len(results)\n",
    "                print(f\"\\n预测为正例的样本: {pos_count} ({pos_count/total:.2%})\")\n",
    "    \n",
    "    print(\"✅ 部署完成\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 运行模型部署\n",
    "# 可修改以下参数\n",
    "model_file = None  # 例如: \"funnel_models/final_model.pkl\"\n",
    "data_file = None   # 例如: \"merged_features.csv\" 或 \"new_data.csv\"\n",
    "target = 'label_apply'  # 如果数据中有目标变量，可以对模型进行评估\n",
    "features_file = None  # 例如: \"funnel_models/selected_features.txt\"\n",
    "key_column = 'input_key'  # 用于批量预测的键列\n",
    "output_file = None  # 预测结果输出文件\n",
    "threshold = 0.5  # 分类阈值\n",
    "\n",
    "deployment_results = run_model_deployment(\n",
    "    model_file, data_file, target, features_file, \n",
    "    key_column, output_file, threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 综合工作流演示\n",
    "\n",
    "以下展示一个完整的工作流程示例，从特征合并到模型部署。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 完整流程示例\n",
    "def run_complete_workflow(target='label_apply'):\n",
    "    \"\"\"\n",
    "    运行完整的工作流程：\n",
    "    1. 合并特征文件\n",
    "    2. 训练模型\n",
    "    3. 超参数调优\n",
    "    4. 模型部署\n",
    "    \"\"\"\n",
    "    print(\"======================\")\n",
    "    print(\"🚀 启动完整工作流程\")\n",
    "    print(\"======================\")\n",
    "    \n",
    "    # 步骤1: 合并特征文件\n",
    "    print(\"\\n📁 步骤1: 合并特征文件\")\n",
    "    merged_df = run_merge_features()\n",
    "    if merged_df is None:\n",
    "        print(\"❌ 特征合并失败，工作流程终止\")\n",
    "        return\n",
    "    \n",
    "    # 步骤2: 训练模型\n",
    "    print(\"\\n🔧 步骤2: 训练模型\")\n",
    "    training_results = run_train_model(target=target)\n",
    "    if training_results is None:\n",
    "        print(\"❌ 模型训练失败，工作流程终止\")\n",
    "        return\n",
    "    \n",
    "    # 步骤3: 超参数调优\n",
    "    print(\"\\n⚙️ 步骤3: 超参数调优\")\n",
    "    tuning_results = run_hyperparameter_tuning(target=target, max_evals=10)  # 降低评估次数以加快示例\n",
    "    if tuning_results is None:\n",
    "        print(\"❌ 超参数调优失败，工作流程继续但使用原始模型\")\n",
    "    \n",
    "    # 步骤4: 模型部署\n",
    "    print(\"\\n🚀 步骤4: 模型部署\")\n",
    "    deployment_results = run_model_deployment(target=target)\n",
    "    \n",
    "    print(\"\\n======================\")\n",
    "    print(\"✅ 工作流程完成\")\n",
    "    print(\"======================\")\n",
    "    \n",
    "    return {\n",
    "        'training': training_results,\n",
    "        'tuning': tuning_results,\n",
    "        'deployment': deployment_results\n",
    "    }\n",
    "\n",
    "# 取消下面的注释来运行完整工作流程\n",
    "# workflow_results = run_complete_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}