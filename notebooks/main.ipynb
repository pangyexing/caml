{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®¢æˆ·è½¬åŒ–æ¨¡å‹ - æ‰§è¡Œæµç¨‹\n",
    "\n",
    "è¿™ä¸ªç¬”è®°æœ¬æä¾›äº†æ‰§è¡Œ `main.py` ä¸­å„ä¸ªåŠŸèƒ½æ¨¡å—çš„äº¤äº’å¼ç•Œé¢ï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æ£€æŸ¥ç‰¹å¾æ–‡ä»¶** - æ£€æŸ¥ç‰¹å¾æ–‡ä»¶æ˜¯å¦æœ‰é‡å¤\n",
    "2. **åˆå¹¶ç‰¹å¾æ–‡ä»¶** - åˆå¹¶å¤šä¸ªç‰¹å¾æ–‡ä»¶\n",
    "3. **è®­ç»ƒæ¨¡å‹** - è®­ç»ƒå®¢æˆ·è½¬åŒ–æ¨¡å‹\n",
    "4. **è¶…å‚æ•°è°ƒä¼˜** - ä¼˜åŒ–æ¨¡å‹å‚æ•°\n",
    "5. **éƒ¨ç½²æ¨¡å‹** - éƒ¨ç½²æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ç¯å¢ƒè®¾ç½®\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional, Union, Any\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„\n",
    "notebook_dir = Path.cwd()\n",
    "project_dir = notebook_dir.parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "# å¯¼å…¥é¡¹ç›®æ¨¡å—\n",
    "from src.core.preprocessing import (\n",
    "    check_feature_files,\n",
    "    merge_feature_files,\n",
    "    preprocess_data,\n",
    ")\n",
    "from src.models.deployment import batch_prediction, deploy_model, load_feature_list\n",
    "from src.models.hyperopt_tuning import hyperopt_xgb, plot_optimization_results\n",
    "from src.models.training import two_stage_modeling_pipeline\n",
    "\n",
    "# è®¾ç½®å¯è§†åŒ–å‚æ•°\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# æ˜¾ç¤ºé¡¹ç›®æ•°æ®ç›®å½•ç»“æ„\n",
    "def list_directory(path, indent=0):\n",
    "    \"\"\"åˆ—å‡ºç›®å½•ç»“æ„\"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"è·¯å¾„ä¸å­˜åœ¨: {path}\")\n",
    "        return\n",
    "    \n",
    "    items = list(path.iterdir())\n",
    "    for item in sorted(items, key=lambda x: (not x.is_dir(), x.name)):\n",
    "        if item.name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        prefix = \"    \" * indent\n",
    "        if item.is_dir():\n",
    "            print(f\"{prefix}ğŸ“ {item.name}/\")\n",
    "            if indent < 2:  # é™åˆ¶é€’å½’æ·±åº¦\n",
    "                list_directory(item, indent + 1)\n",
    "        else:\n",
    "            size_mb = item.stat().st_size / (1024 * 1024)\n",
    "            if size_mb > 1:\n",
    "                size_str = f\"{size_mb:.1f}MB\"\n",
    "            else:\n",
    "                size_kb = item.stat().st_size / 1024\n",
    "                size_str = f\"{size_kb:.1f}KB\"\n",
    "            print(f\"{prefix}ğŸ“„ {item.name} ({size_str})\")\n",
    "\n",
    "print(\"é¡¹ç›®ç›®å½•ç»“æ„:\")\n",
    "list_directory(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ£€æŸ¥ç‰¹å¾æ–‡ä»¶\n",
    "\n",
    "æ£€æŸ¥ç‰¹å¾æ–‡ä»¶æ˜¯å¦æœ‰é‡å¤ç‰¹å¾æˆ–å…¶ä»–é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_check_features(feature_files=None):\n",
    "    \"\"\"æ£€æŸ¥ç‰¹å¾æ–‡ä»¶\"\"\"\n",
    "    if feature_files is None:\n",
    "        # å¦‚æœæœªæä¾›æ–‡ä»¶åˆ—è¡¨ï¼Œå°è¯•æŸ¥æ‰¾æ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶\n",
    "        data_dir = project_dir / \"data\"\n",
    "        if data_dir.exists():\n",
    "            feature_files = list(data_dir.glob(\"*.csv\"))\n",
    "            feature_files = [str(f) for f in feature_files]\n",
    "            print(f\"æ‰¾åˆ° {len(feature_files)} ä¸ªç‰¹å¾æ–‡ä»¶:\")\n",
    "            for f in feature_files:\n",
    "                print(f\"  - {f}\")\n",
    "        else:\n",
    "            print(f\"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}\")\n",
    "            return\n",
    "    \n",
    "    if not feature_files:\n",
    "        print(\"æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾æ–‡ä»¶\")\n",
    "        return\n",
    "    \n",
    "    print(f\"æ£€æŸ¥ {len(feature_files)} ä¸ªç‰¹å¾æ–‡ä»¶æ˜¯å¦æœ‰é‡å¤...\")\n",
    "    results = check_feature_files(feature_files)\n",
    "    \n",
    "    if results['status'] == 'success':\n",
    "        print(\"âœ… æ²¡æœ‰å‘ç°é—®é¢˜\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ å‘ç° {len(results['issues'])} ä¸ªé—®é¢˜:\")\n",
    "        for issue in results['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# è¿è¡Œç‰¹å¾æ–‡ä»¶æ£€æŸ¥\n",
    "# å¦‚æœéœ€è¦æŒ‡å®šæ–‡ä»¶ï¼Œè¯·æ›¿æ¢ä¸‹é¢çš„None\n",
    "feature_files = None  # ä¾‹å¦‚: [\"data/feature1.csv\", \"data/feature2.csv\"]\n",
    "check_results = run_check_features(feature_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åˆå¹¶ç‰¹å¾æ–‡ä»¶\n",
    "\n",
    "å°†å¤šä¸ªç‰¹å¾æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªæ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_merge_features(feature_files=None, sample_file=None, output_file=None):\n",
    "    \"\"\"åˆå¹¶ç‰¹å¾æ–‡ä»¶\"\"\"\n",
    "    if feature_files is None:\n",
    "        # å¦‚æœæœªæä¾›æ–‡ä»¶åˆ—è¡¨ï¼Œå°è¯•æŸ¥æ‰¾æ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶\n",
    "        data_dir = project_dir / \"data\"\n",
    "        if data_dir.exists():\n",
    "            feature_files = list(data_dir.glob(\"*.csv\"))\n",
    "            feature_files = [str(f) for f in feature_files]\n",
    "            print(f\"æ‰¾åˆ° {len(feature_files)} ä¸ªç‰¹å¾æ–‡ä»¶:\")\n",
    "            for f in feature_files:\n",
    "                print(f\"  - {f}\")\n",
    "        else:\n",
    "            print(f\"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}\")\n",
    "            return\n",
    "    \n",
    "    if not feature_files:\n",
    "        print(\"æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾æ–‡ä»¶\")\n",
    "        return\n",
    "    \n",
    "    print(f\"åˆå¹¶ {len(feature_files)} ä¸ªç‰¹å¾æ–‡ä»¶...\")\n",
    "    merged_df = merge_feature_files(feature_files, sample_file)\n",
    "    \n",
    "    # ä¿å­˜åˆå¹¶çš„æ•°æ®é›†\n",
    "    if output_file is None:\n",
    "        output_file = str(project_dir / \"merged_features.csv\")\n",
    "    \n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… å·²å°†åˆå¹¶åçš„æ•°æ®é›†ä¿å­˜åˆ° {output_file}\")\n",
    "    print(f\"æ•°æ®é›†å¤§å°: {len(merged_df)} è¡Œ x {len(merged_df.columns)} åˆ—\")\n",
    "    \n",
    "    # æ˜¾ç¤ºä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(\"\\næ•°æ®é›†æ¦‚è§ˆ:\")\n",
    "    display(merged_df.head())\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# è¿è¡Œç‰¹å¾æ–‡ä»¶åˆå¹¶\n",
    "# å¦‚éœ€æŒ‡å®šç‰¹å®šæ–‡ä»¶ï¼Œè¯·ä¿®æ”¹ä¸‹é¢çš„å‚æ•°\n",
    "feature_files = None  # ä¾‹å¦‚: [\"data/feature1.csv\", \"data/feature2.csv\"]\n",
    "sample_file = None    # ä¾‹å¦‚: \"data/sample.csv\"\n",
    "output_file = None    # ä¾‹å¦‚: \"merged_data.csv\"\n",
    "\n",
    "merged_df = run_merge_features(feature_files, sample_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "è®­ç»ƒå®¢æˆ·è½¬åŒ–æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_train_model(data_file=None, target='label_apply', resume_from=None):\n",
    "    \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "    if data_file is None:\n",
    "        # å¦‚æœæœªæä¾›æ•°æ®æ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨åˆå¹¶çš„ç‰¹å¾æ–‡ä»¶\n",
    "        default_data = project_dir / \"merged_features.csv\"\n",
    "        if default_data.exists():\n",
    "            data_file = str(default_data)\n",
    "        else:\n",
    "            print(f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {default_data}\")\n",
    "            print(\"è¯·å…ˆåˆå¹¶ç‰¹å¾æ–‡ä»¶æˆ–æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„\")\n",
    "            return\n",
    "    \n",
    "    print(f\"æ­£åœ¨ä» {data_file} åŠ è½½æ•°æ®...\")\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    if data_file.endswith('.csv'):\n",
    "        df = pd.read_csv(data_file)\n",
    "    elif data_file.endswith('.parquet'):\n",
    "        df = pd.read_parquet(data_file)\n",
    "    else:\n",
    "        print(f\"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œå¤§å°: {df.shape}\")\n",
    "    \n",
    "    # æ£€æŸ¥ç›®æ ‡å˜é‡\n",
    "    if target not in df.columns:\n",
    "        print(f\"é”™è¯¯: ç›®æ ‡å˜é‡ '{target}' ä¸åœ¨æ•°æ®é›†ä¸­\")\n",
    "        print(f\"å¯ç”¨åˆ—: {', '.join(df.columns[:10])}...\")\n",
    "        return\n",
    "    \n",
    "    # é¢„å¤„ç†å’Œæ‹†åˆ†æ•°æ®\n",
    "    print(\"é¢„å¤„ç†æ•°æ®...\")\n",
    "    train_df, test_df = preprocess_data(df, target=target)\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›†å¤§å°: {train_df.shape}\")\n",
    "    print(f\"æµ‹è¯•é›†å¤§å°: {test_df.shape}\")\n",
    "    \n",
    "    # è¿è¡Œä¸¤é˜¶æ®µå»ºæ¨¡æµç¨‹\n",
    "    print(\"å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
    "    results = two_stage_modeling_pipeline(\n",
    "        train_df, \n",
    "        test_df, \n",
    "        target=target,\n",
    "        resume_from=resume_from\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… è®­ç»ƒå®Œæˆ\")\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœæ‘˜è¦\n",
    "    if isinstance(results, dict) and 'metrics' in results:\n",
    "        print(\"\\næ¨¡å‹è¯„ä¼°æŒ‡æ ‡:\")\n",
    "        for metric, value in results['metrics'].items():\n",
    "            print(f\"  - {metric}: {value}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# è¿è¡Œæ¨¡å‹è®­ç»ƒ\n",
    "# å¯ä¿®æ”¹ä»¥ä¸‹å‚æ•°\n",
    "data_file = None  # ä¾‹å¦‚: \"merged_features.csv\"\n",
    "target = 'label_apply'  # ç›®æ ‡å˜é‡åˆ—å\n",
    "resume_from = None  # å¯é€‰: 'start', 'initial_model', 'feature_analysis', 'feature_selection', 'final_model'\n",
    "\n",
    "training_results = run_train_model(data_file, target, resume_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è¶…å‚æ•°è°ƒä¼˜\n",
    "\n",
    "å¯¹æ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_hyperparameter_tuning(data_file=None, target='label_apply', max_evals=50):\n",
    "    \"\"\"è¶…å‚æ•°è°ƒä¼˜\"\"\"\n",
    "    # æ£€æŸ¥ç‰¹å¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    features_file = project_dir / \"funnel_models\" / \"selected_features.txt\"\n",
    "    \n",
    "    if not features_file.exists():\n",
    "        print(f\"é”™è¯¯: ç‰¹å¾åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {features_file}\")\n",
    "        print(\"è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼\")\n",
    "        return\n",
    "    \n",
    "    # å†³å®šæ˜¯åŠ è½½åŸå§‹æ•°æ®è¿˜æ˜¯ä½¿ç”¨å·²æœ‰çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®\n",
    "    if data_file:\n",
    "        print(f\"ä» {data_file} åŠ è½½æ•°æ®...\")\n",
    "        \n",
    "        # åŠ è½½æ•°æ®\n",
    "        if data_file.endswith('.csv'):\n",
    "            df = pd.read_csv(data_file)\n",
    "        elif data_file.endswith('.parquet'):\n",
    "            df = pd.read_parquet(data_file)\n",
    "        else:\n",
    "            print(f\"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_file}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œå¤§å°: {df.shape}\")\n",
    "        \n",
    "        # é¢„å¤„ç†å’Œæ‹†åˆ†æ•°æ®\n",
    "        print(\"é¢„å¤„ç†æ•°æ®...\")\n",
    "        train_df, test_df = preprocess_data(df, target=target)\n",
    "    else:\n",
    "        print(\"å°è¯•ä» funnel_models ç›®å½•åŠ è½½å·²å¤„ç†çš„æ•°æ®...\")\n",
    "        \n",
    "        # å°è¯•åŠ è½½ä¿å­˜çš„æ–‡ä»¶\n",
    "        train_file = project_dir / \"funnel_models\" / \"train.csv\"\n",
    "        test_file = project_dir / \"funnel_models\" / \"test.csv\"\n",
    "        \n",
    "        if not train_file.exists() or not test_file.exists():\n",
    "            print(f\"é”™è¯¯: è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_file} æˆ– {test_file}\")\n",
    "            print(\"è¯·æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„æˆ–å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼\")\n",
    "            return\n",
    "        \n",
    "        train_df = pd.read_csv(train_file)\n",
    "        test_df = pd.read_csv(test_file)\n",
    "        \n",
    "        print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œè®­ç»ƒé›†: {train_df.shape}, æµ‹è¯•é›†: {test_df.shape}\")\n",
    "    \n",
    "    # åŠ è½½ç‰¹å¾åˆ—è¡¨\n",
    "    features = load_feature_list(str(features_file))\n",
    "    print(f\"ä» {features_file} åŠ è½½äº† {len(features)} ä¸ªç‰¹å¾\")\n",
    "    \n",
    "    # è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–\n",
    "    print(f\"å¼€å§‹è¶…å‚æ•°ä¼˜åŒ– (æœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evals})...\")\n",
    "    results = hyperopt_xgb(\n",
    "        train_df,\n",
    "        test_df,\n",
    "        features,\n",
    "        target=target,\n",
    "        max_evals=max_evals\n",
    "    )\n",
    "    \n",
    "    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_dir = project_dir / \"optimization_results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    results_file = results_dir / f\"{target}_{timestamp}_results.json\"\n",
    "    print(f\"ç»˜åˆ¶ä¼˜åŒ–ç»“æœå›¾è¡¨ï¼Œä¿å­˜åˆ° {results_file}...\")\n",
    "    plot_optimization_results(str(results_file))\n",
    "    \n",
    "    print(\"âœ… è¶…å‚æ•°è°ƒä¼˜å®Œæˆ\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæœ€ä½³å‚æ•°\n",
    "    if 'best_params' in results:\n",
    "        print(\"\\næœ€ä½³å‚æ•°:\")\n",
    "        for param, value in results['best_params'].items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "        \n",
    "        if 'best_score' in results:\n",
    "            print(f\"\\næœ€ä½³å¾—åˆ†: {results['best_score']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# è¿è¡Œè¶…å‚æ•°è°ƒä¼˜\n",
    "# å¯ä¿®æ”¹ä»¥ä¸‹å‚æ•°\n",
    "data_file = None  # å¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨ funnel_models ä¸­çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®\n",
    "target = 'label_apply'  # ç›®æ ‡å˜é‡åˆ—å\n",
    "max_evals = 50  # æœ€å¤§è¯„ä¼°æ¬¡æ•°ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "\n",
    "tuning_results = run_hyperparameter_tuning(data_file, target, max_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. éƒ¨ç½²æ¨¡å‹\n",
    "\n",
    "éƒ¨ç½²æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_model_deployment(model_file=None, data_file=None, target=None, features_file=None, \n",
    "                         key_column='input_key', output_file=None, threshold=0.5):\n",
    "    \"\"\"éƒ¨ç½²æ¨¡å‹\"\"\"\n",
    "    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶\n",
    "    if model_file is None:\n",
    "        # å°è¯•æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶\n",
    "        model_dir = project_dir / \"funnel_models\"\n",
    "        if model_dir.exists() and (model_dir / \"final_model.pkl\").exists():\n",
    "            model_file = str(model_dir / \"final_model.pkl\")\n",
    "        else:\n",
    "            # ä¹Ÿå¯ä»¥æŸ¥æ‰¾tuned_modelsç›®å½•\n",
    "            tuned_dir = project_dir / \"tuned_models\"\n",
    "            if tuned_dir.exists():\n",
    "                model_files = list(tuned_dir.glob(\"*.pkl\"))\n",
    "                if model_files:\n",
    "                    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„\n",
    "                    model_file = str(sorted(model_files, key=lambda x: x.stat().st_mtime, reverse=True)[0])\n",
    "    \n",
    "    if model_file is None or not os.path.exists(model_file):\n",
    "        print(f\"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "        print(\"è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æŒ‡å®šæ¨¡å‹æ–‡ä»¶è·¯å¾„\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ä½¿ç”¨æ¨¡å‹: {model_file}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
    "    if data_file is None:\n",
    "        # å°è¯•ä½¿ç”¨åˆå¹¶çš„ç‰¹å¾æ–‡ä»¶\n",
    "        default_data = project_dir / \"merged_features.csv\"\n",
    "        if default_data.exists():\n",
    "            data_file = str(default_data)\n",
    "        else:\n",
    "            print(f\"é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "            print(\"è¯·æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„\")\n",
    "            return\n",
    "    \n",
    "    print(f\"ä» {data_file} åŠ è½½æ•°æ®...\")\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    if data_file.endswith('.csv'):\n",
    "        df = pd.read_csv(data_file)\n",
    "    elif data_file.endswith('.parquet'):\n",
    "        df = pd.read_parquet(data_file)\n",
    "    else:\n",
    "        print(f\"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œå¤§å°: {df.shape}\")\n",
    "    \n",
    "    # æ£€æŸ¥ç‰¹å¾æ–‡ä»¶\n",
    "    if features_file is None:\n",
    "        default_features = project_dir / \"funnel_models\" / \"selected_features.txt\"\n",
    "        if default_features.exists():\n",
    "            features_file = str(default_features)\n",
    "        else:\n",
    "            print(f\"é”™è¯¯: ç‰¹å¾åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "            print(\"è¯·æŒ‡å®šç‰¹å¾åˆ—è¡¨æ–‡ä»¶è·¯å¾„\")\n",
    "            return\n",
    "    \n",
    "    # éƒ¨ç½²æ¨¡å‹\n",
    "    if target and target in df.columns:\n",
    "        print(f\"éƒ¨ç½²æ¨¡å‹å¹¶å¯¹ {target} è¿›è¡Œè¯„ä¼°...\")\n",
    "        \n",
    "        results = deploy_model(\n",
    "            model_file,\n",
    "            df,\n",
    "            target=target,\n",
    "            features_file=features_file,\n",
    "            threshold=threshold\n",
    "        )\n",
    "        \n",
    "        # æ˜¾ç¤ºè¯„ä¼°ç»“æœ\n",
    "        if isinstance(results, dict) and 'metrics' in results:\n",
    "            print(\"\\næ¨¡å‹è¯„ä¼°æŒ‡æ ‡:\")\n",
    "            for metric, value in results['metrics'].items():\n",
    "                print(f\"  - {metric}: {value}\")\n",
    "    else:\n",
    "        print(\"éƒ¨ç½²æ¨¡å‹è¿›è¡Œæ‰¹é‡é¢„æµ‹...\")\n",
    "        \n",
    "        if output_file is None:\n",
    "            # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶\n",
    "            deploy_dir = project_dir / \"deployment_results\"\n",
    "            deploy_dir.mkdir(exist_ok=True)\n",
    "            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_file = str(deploy_dir / f\"predictions_{timestamp}.csv\")\n",
    "        \n",
    "        results = batch_prediction(\n",
    "            model_file,\n",
    "            df,\n",
    "            key_column=key_column,\n",
    "            features_file=features_file,\n",
    "            output_file=output_file,\n",
    "            threshold=threshold\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… é¢„æµ‹å®Œæˆï¼Œç»“æœä¿å­˜åˆ° {output_file}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºé¢„æµ‹ç»“æœé¢„è§ˆ\n",
    "        if isinstance(results, pd.DataFrame):\n",
    "            print(\"\\né¢„æµ‹ç»“æœé¢„è§ˆ:\")\n",
    "            display(results.head())\n",
    "            \n",
    "            # ç»Ÿè®¡é¢„æµ‹æƒ…å†µ\n",
    "            if 'prediction' in results.columns:\n",
    "                pos_count = results['prediction'].sum()\n",
    "                total = len(results)\n",
    "                print(f\"\\né¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬: {pos_count} ({pos_count/total:.2%})\")\n",
    "    \n",
    "    print(\"âœ… éƒ¨ç½²å®Œæˆ\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# è¿è¡Œæ¨¡å‹éƒ¨ç½²\n",
    "# å¯ä¿®æ”¹ä»¥ä¸‹å‚æ•°\n",
    "model_file = None  # ä¾‹å¦‚: \"funnel_models/final_model.pkl\"\n",
    "data_file = None   # ä¾‹å¦‚: \"merged_features.csv\" æˆ– \"new_data.csv\"\n",
    "target = 'label_apply'  # å¦‚æœæ•°æ®ä¸­æœ‰ç›®æ ‡å˜é‡ï¼Œå¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°\n",
    "features_file = None  # ä¾‹å¦‚: \"funnel_models/selected_features.txt\"\n",
    "key_column = 'input_key'  # ç”¨äºæ‰¹é‡é¢„æµ‹çš„é”®åˆ—\n",
    "output_file = None  # é¢„æµ‹ç»“æœè¾“å‡ºæ–‡ä»¶\n",
    "threshold = 0.5  # åˆ†ç±»é˜ˆå€¼\n",
    "\n",
    "deployment_results = run_model_deployment(\n",
    "    model_file, data_file, target, features_file, \n",
    "    key_column, output_file, threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»¼åˆå·¥ä½œæµæ¼”ç¤º\n",
    "\n",
    "ä»¥ä¸‹å±•ç¤ºä¸€ä¸ªå®Œæ•´çš„å·¥ä½œæµç¨‹ç¤ºä¾‹ï¼Œä»ç‰¹å¾åˆå¹¶åˆ°æ¨¡å‹éƒ¨ç½²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# å®Œæ•´æµç¨‹ç¤ºä¾‹\n",
    "def run_complete_workflow(target='label_apply'):\n",
    "    \"\"\"\n",
    "    è¿è¡Œå®Œæ•´çš„å·¥ä½œæµç¨‹ï¼š\n",
    "    1. åˆå¹¶ç‰¹å¾æ–‡ä»¶\n",
    "    2. è®­ç»ƒæ¨¡å‹\n",
    "    3. è¶…å‚æ•°è°ƒä¼˜\n",
    "    4. æ¨¡å‹éƒ¨ç½²\n",
    "    \"\"\"\n",
    "    print(\"======================\")\n",
    "    print(\"ğŸš€ å¯åŠ¨å®Œæ•´å·¥ä½œæµç¨‹\")\n",
    "    print(\"======================\")\n",
    "    \n",
    "    # æ­¥éª¤1: åˆå¹¶ç‰¹å¾æ–‡ä»¶\n",
    "    print(\"\\nğŸ“ æ­¥éª¤1: åˆå¹¶ç‰¹å¾æ–‡ä»¶\")\n",
    "    merged_df = run_merge_features()\n",
    "    if merged_df is None:\n",
    "        print(\"âŒ ç‰¹å¾åˆå¹¶å¤±è´¥ï¼Œå·¥ä½œæµç¨‹ç»ˆæ­¢\")\n",
    "        return\n",
    "    \n",
    "    # æ­¥éª¤2: è®­ç»ƒæ¨¡å‹\n",
    "    print(\"\\nğŸ”§ æ­¥éª¤2: è®­ç»ƒæ¨¡å‹\")\n",
    "    training_results = run_train_model(target=target)\n",
    "    if training_results is None:\n",
    "        print(\"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œå·¥ä½œæµç¨‹ç»ˆæ­¢\")\n",
    "        return\n",
    "    \n",
    "    # æ­¥éª¤3: è¶…å‚æ•°è°ƒä¼˜\n",
    "    print(\"\\nâš™ï¸ æ­¥éª¤3: è¶…å‚æ•°è°ƒä¼˜\")\n",
    "    tuning_results = run_hyperparameter_tuning(target=target, max_evals=10)  # é™ä½è¯„ä¼°æ¬¡æ•°ä»¥åŠ å¿«ç¤ºä¾‹\n",
    "    if tuning_results is None:\n",
    "        print(\"âŒ è¶…å‚æ•°è°ƒä¼˜å¤±è´¥ï¼Œå·¥ä½œæµç¨‹ç»§ç»­ä½†ä½¿ç”¨åŸå§‹æ¨¡å‹\")\n",
    "    \n",
    "    # æ­¥éª¤4: æ¨¡å‹éƒ¨ç½²\n",
    "    print(\"\\nğŸš€ æ­¥éª¤4: æ¨¡å‹éƒ¨ç½²\")\n",
    "    deployment_results = run_model_deployment(target=target)\n",
    "    \n",
    "    print(\"\\n======================\")\n",
    "    print(\"âœ… å·¥ä½œæµç¨‹å®Œæˆ\")\n",
    "    print(\"======================\")\n",
    "    \n",
    "    return {\n",
    "        'training': training_results,\n",
    "        'tuning': tuning_results,\n",
    "        'deployment': deployment_results\n",
    "    }\n",
    "\n",
    "# å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥è¿è¡Œå®Œæ•´å·¥ä½œæµç¨‹\n",
    "# workflow_results = run_complete_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}